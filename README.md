# Log-Analytic-with-pyspark

Log Analytics with PySpark
A hands-on data engineering project for analyzing large-scale log files using Apache Spark (PySpark). This project demonstrates how to extract, process, and visualize insights from raw web server logs efficiently, leveraging the power of distributed computing.

Key Features:
- Parse and process large Apache log files using PySpark for scalable analytics.
- Extract key fields such as IP address, timestamp, HTTP method, endpoint, and status code with regular expressions.

- Generate insightful analytics including:
  - Status code distribution (e.g., 200, 404, 500)
  - Top 10 most frequent IP addresses
  - Top 10 most requested endpoints
  - Daily traffic trends

- Visualize results with matplotlib (bar chart, line chart).
- Export analytics data to CSV files for further use or reporting.

How this project can be extended:
- Analyze user agent or referrer fields for deeper insights.
- Detect suspicious activity or security threats from log patterns.
- Integrate with real-time streaming (Spark Structured Streaming).
- Build interactive dashboards (e.g., with Streamlit or Metabase).

<img width="148" alt="image" src="https://github.com/user-attachments/assets/fd3e3099-b89c-42e9-923c-c8cef40123df" />
<img width="183" alt="image" src="https://github.com/user-attachments/assets/40fbc4d6-56e1-4090-94d7-ba06932fba8b" />
<img width="273" alt="image" src="https://github.com/user-attachments/assets/623cef56-bfb3-47fa-8609-c8036d03fcc6" />
<img width="592" alt="image" src="https://github.com/user-attachments/assets/f4161482-ec4e-42c0-9550-f433ef2f403e" />
